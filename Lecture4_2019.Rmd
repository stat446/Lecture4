---
title: "Lecture 4"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
```


\section*{Simple Random Sampling}
**Simple random sampling without replacement** of size $n$ 
\vfill

Some necessary combinatorial notation:

- (n factorial) $n! = n \times (n-1) \times (n-2) \times \dots \times 2 \times 1$. This is the number of unique arrangements or orderings (or permutations) of $n$ distinct items.
\vfill
- (N choose n) $\binom{N}{n} = \frac{N!}{n!(N-n)!} = \frac{N(N-1) \dots (N-n+1)}{n!}.$ This is the number of combinations of $n$ items selected from $N$ distinct items (and the order of selection doesn't matter). 
\vfill

- There are $\binom{N}{n}$ possible SRSs of size $n$ from a population of size N. Each has $P(\mathcal{S}) = \frac{1}{\binom{N}{n}}$
\vfill

In general, we will assume sampling is without replacement.
\vfill

##### Estimation of $\bar{y}_U$ and $t$
Using the resampling approach from Lab 2, we could calculate variability or construct confidence intervals of our estimators from the approximate sampling distribution created in R. 
\vfill

\newpage
A natural **estimator** for the population mean $\bar{y}_U$ is the sample mean $\bar{y}.$ 
	\begin{equation*}
	\hat{\bar{y}}_U = \bar{y} = \frac{1}{n} \sum_{i=1}^n y_i \hspace{4cm} 
	\end{equation*}

\vfill
$\hat{\bar{y}}_U$ and $\hat{t}$ are  **design unbiased**. 

Does design unbiased imply anything about the variation of the estimator?
\vfill

The next problem is to 

You have probably been taught that the variance of the sample mean $V(\bar{Y}) = S^2/n$ and this is appropriate for samples from very large or even infinite populations. 

Ex. Consider an extreme case where $n=N$, that is all units are sampled. What is the variability in $\bar{y}$?
\vfill



When dealing with **finite populations** that are not very large, we adjust the variance formulas using the \vfill
Then with the use of FPC the variance formulas are:
	\begin{equation*}
	*V(\hat{\bar{y}}_U) = \frac{S^2}{n} \left(1-\frac{n}{N} \right) = \frac{S^2}{n} \left(\frac{N-n}{N} \right) \hspace{3cm} V(\hat{t})=N^2 	V(\hat{\bar{y}}_U) = N(N-n)\frac{S^2}{n}*
	\end{equation*}

\vfill
Using the estimated variance $s^2$ in place of $S^2$ in the above formulas and taking the square root gives the *standard error (SE)* of these estimators.
\vfill

Notes about the FPC:

- If N is large relative to n, then the FPC will be 
\vfill
- If N is not large relative to n, then omitting the FPC from the variance formulas can 
\vfill
- As $n \rightarrow N, \frac{N-n}{N}$ 
\vfill
- Thinking about this deeper, where does the uncertainty in $\bar{y}$ come from?
\vfill
\newpage

The **Coefficient of Variation (CV)** of an estimator $\bar{y}$ measures \vfill
	\begin{equation*}
	CV(\bar{y}) = \frac{\sqrt{V(\bar{y})}}{E(\bar{y})} = \sqrt{1-\frac{n}{N}}\frac{S}{\sqrt{n}}\times\bar{y}_U
	\end{equation*}
	Note the CV does not have a unit of measurement.
\vfill

\vfill
The estimated CV uses the standard error divided by the sample mean

\vfill

##### Confidence Intervals for $\bar{y}_U$ and $t$
Similar to confidence intervals you have seen in other statistics courses, we will use asymptotics and the central limit theorem to construct confidence intervals.
\vfill
In an introductory statistics course, you were given confidence interval formulas
 	\begin{equation*}
 	\bar{y} \pm  \hspace{3cm} \text{and} \hspace{3cm} \bar{y} \pm 
 	\end{equation*}
 	These formulas are applicable if a sample was to be taken from an infinitely or extremely large population. But when we are dealing with finite populations, 
 	\vfill
In the finite population version of the Central Limit Theorem, we assume the estimators $\hat{\bar{y}}_U=\bar{y}$ and $\hat{t}=N\bar{y}$ have sampling distributions that are approximately normal. That is,
 \vfill
 	\vfill
For large samples, approximate 100(1-$\alpha$)\% confidence intervals for $\bar{y}_U$ and $t$ are:
 	\begin{eqnarray*}
 	\text{For } \bar{y}_U \hspace{4cm}&\hspace{2cm} & \text{For } t:\hspace{4cm}\\
 	\bar{y} \pm z^* \sqrt{\left(\frac{N-n}{N} \right) \frac{s^2}{n}} && N\bar{y} \pm z^* \sqrt{N(N-n)\frac{s^2}{n}}\\
 	 	\bar{y} \pm z^*s \sqrt{\left(\frac{N-n}{N} \right)/n} && N\bar{y} \pm z^*s \sqrt{N(N-n)/n},
 	\end{eqnarray*}
 	where $z^*$ is the upper $\alpha$/2 critical value from the standard normal distirbution
\vfill
\newpage
For smaller samples, approximate 100(1-$\alpha$)\% confidence intervals for $\bar{y}_U$ and $t$ are:
 	\begin{eqnarray*}
 	\text{For } \bar{y}_U \hspace{4cm}&\hspace{2cm} & \text{For } t:\hspace{4cm}\\
 	\bar{y} \pm t^* \sqrt{\left(\frac{N-n}{N} \right) \frac{s^2}{n}} && N\bar{y} \pm t^* \sqrt{N(N-n)\frac{s^2}{n}}\\
 	 	\bar{y} \pm t^*s \sqrt{\left(\frac{N-n}{N} \right)/n} && N\bar{y} \pm t^*s \sqrt{N(N-n)/n},
 	\end{eqnarray*}
	where $t^*$ is the upper $\alpha$/2 critical values from the $t$ distribution with $n-1$ degrees of freedom.
\vfill
So what constitutes a large sample?
\vfill

The quantity being added and subtracted from $\hat{\bar{y}}_U=\bar{y}$ or $\hat{t} = N\bar{y}$ in the confidence interval is known as the 
\vfill

##### One-sided Confidence Intervals for $\bar{y}_U$ and $t$
Occasionally, a researcher may want a one-sided confidence interval. 
\vfill

Approximate \emph{upper} and \emph{lower} 100(1-$\alpha$)\% confidence intervals for $\bar{y}_U$ and $t$ are:
	\begin{eqnarray*}
		\text{For } \bar{y}_U \hspace{4cm}&\hspace{2cm} & \text{For } t:\hspace{4cm}\\
 \text{\bf upper} \hspace{3cm}	\left( \bar{y} - t^*s \sqrt{\left(\frac{N-n}{N}\right)/n},\infty \right)&& \left( N\bar{y} - t^*s \sqrt{N(N-n)/n},\infty \right)\\
 	 \text{\bf lower} \hspace{3cm}		\left(-\infty, \bar{y} + t^*s \sqrt{\left(\frac{N-n}{N}\right)/n} \right)&& \left(-\infty, N\bar{y} + t^*s \sqrt{N(N-n)/n},\infty \right),
 	 	\end{eqnarray*}
 	 	where $t^*$ is the upper $\alpha$ critical value from a $t(n-1)$ distribution. For larger samples, $t^*$ can be replaced with $z^*$*
 \vfill
 
 If the response (y-values) cannot be negative, replace $-\infty$ with 0 in the lower confidence interval formulas. If the response cannot be positive, replace $\infty$ with 0 in the upper confidence formulas.
 \vfill
 Later, we will discuss another method of generating a confidence interval 
\newpage

##### Confidence Intervals in R
There are `R` packages available, but writing code (and functions) in `R` to compute confidence intervals is straightforward. The code below generates a population of data from a Poisson distribution and samples 10 observations.

```{r}
N <- 100
mu <- 15
n <- 10
population <- rpois(N,mu)
sample.responses <- sample(population,n)
y.bar <- mean(sample.responses)
s <- sd(sample.responses)
alpha <- .05
conf.int <- c(y.bar - qt(1-alpha/2, df = n-1)*s*sqrt(((N-n)/N)/n),
              y.bar + qt(1-alpha/2, df = n-1)*s*sqrt(((N-n)/N)/n))

```

The confidence interval for the population mean is (`r round(conf.int,2)[1]`, `r round(conf.int,2)[2]`).
\vfill
Another way to do this is to write a function in R. Here is the same code placed into a function.
```{r}
FPC_ci <- function(sample,N,alpha=.05){
  # FUNCTION TO CALCULATE FPC BASED CONFIDENCE INTERVAL
  # ARGUMENTS:
  #    sample: observed responses
  #    N: total population size
  #    alpha = significance level
  n <- length(sample)
  y.bar <- mean(sample)
  s <- sd(sample)
 conf.int <- c(y.bar - qt(1-alpha/2, df = n-1)*s*sqrt(((N-n)/N)/n),
               y.bar + qt(1-alpha/2, df = n-1)*s*sqrt(((N-n)/N)/n))
 return(conf.int)
}

FPC_ci(sample.responses, 100)
```


\vfill

Calculate the confidence interval again for varying sizes of $n$ and $N$ (this will require resimulating the data).

- How do your results vary?
- What are the implications of changing $n$ or $N$?
\newpage


#### Proportion Estimation

Suppose we are interested in an attribute associated with the sampling units. 
\vfill

Statistically the goal is to estimate proportion $p$. We use an indicator function that assigns a $y_i$ value to unit $i$ as follows:
	\begin{equation*}
	y_i = \begin{cases}
	 1 \text{  if unit $i$ possess the attribute} \\
		 0 \text{  otherwise}
	\end{cases}
	\end{equation*}
Then $t = \sum_{i=1}^N y_i$ and $\bar{y}_U = \frac{1}{N} \sum_{i=1}^N y_i = p$. 
\vfill

By taking a SRS of size $n$, we can estimate $p$ with the **sample proportion** $\hat{p}$ of units that possess that attribute:
	\begin{equation*}
	\hat{p} = \frac{\sum_{i=1}^n y_i}{n} = \bar{y}
	\end{equation*}
	The sampled proportion $\hat{p}$ is unbiased for $p$.
\vfill	

For a finite population of 0 and 1 values, the variance of $\hat{p}$ is
	\begin{equation*}
	V(\hat{p})=\left(\frac{N-n}{N}\right) \frac{S^2}{n} = \left(\frac{N-n}{N} \right) \left( \frac{N}{N-1} \right) \frac{p(1-p)}{n} = \left(\frac{N-n}{N-1} \right) \frac{p(1-p)}{n}
	\end{equation*}
	
Because $S^2$ is unknown, we estimate it with $s^2 = \frac{n}{n-1} 1\hat{p}(1-\hat{p}).$, 
\vfill

Note: the effects of omitting the FPC from the formulas in large and small samples are the same as earlier.


\vfill
##### Confidence intervals for $p$

Let the random variable $Y$ be the number of units in a SRS of size $n$ that possess the attribute of interest. 
\vfill
\newpage

Hypergeometric distribution for SRS: the probability that a SRS of size $n$ will have exactly $j$ sampling units that possess the attribute is:
	\begin{equation*}
	Pr(Y=j) = \frac{\binom{t}{j}\binom{N-t}{n-j}}{\binom{N}{n}}.
	\end{equation*}
	This is the probability that SRS will consist of $j$ ones and $n-j$ zeroes selected from the population containing $t$ ones and $N-t$ zeros.
	\vfill
	

\vfill

Remember there are $t$ ones and N-t zeros in the population. However, t is unknown. 
\vfill

Although the problem no longer depends on t, it still depends on the unknown proportion parameter p. What is commonly done is to apply the normal approximation to the binomial distribution:
	\begin{equation*}
	\hat{p} \overset{.}{\sim} N(p,V(\hat{p})
	\end{equation*}
\vfill

Thus, if the sample size $n$ is large enough, we use $\hat{V}(\hat{p})$ to estimate $V(\hat{p})$. An approximate 100(1-$\alpha$)\% confidence interval for p is:
	\begin{equation*}
		\hat{p} \pm z^* \sqrt{\hat{V}(\hat{p}} \hspace{1.5cm} \text{OR} \hspace{1.5cm} \hat{p} \pm z^* \sqrt{\left(\frac{N-n}{N}\right) \frac{\hat{p}(1-\hat{p})}{n-1}},
	\end{equation*}
	where $z^*$ is the upper $\alpha/2$ critical value from the standard normal distribution. Sample sizes are typically large enough to use $z^*$ instead of $t^*$.
\vfill

The normal approximation will be reasonable given


\vfill
\newpage


### Sample Size Determination with Simple Random Sampling
 It is well known that an increase in sample size $n$ will lead to a more precise estimator of $\bar{y}_U$ or $t$. It is also obvious that an increase in the sample size $n$ will make the sample more expensive to collect. There will, however, be a limited amount of resources available for data collection.
 \vfill
 
When designing a sampling plan, the researcher wants to acheive a desired degree of reliability at the lowest possible cost while satisfying the resource limitations for data collection. 
\vfill

To do this, the researcher tries to achieve a balance to avoid the following mistakes:


\vfill

\vfill

To determine a sample size n when estimating a parameter $\theta$, we do the following:

\vfill


##### Sample Size for $\bar{y}_U$
Goal: Estimate the SRS size required so the probability that the difference between the estimator $\hat{\bar{y}}_U$ and the population mean $\bar{y}_U$ does not exceed a maximum allowable difference $d$ is at most $\alpha$.
\vfill

Mathematically, find $n$ such that 
\vfill

Assuming $\bar{y}$ is approximately normally distributed, this is equivalent to finding n so that the margin of error
	 $$z_{\alpha/2}\sqrt{\left(\frac{N-n}{N}\right) \frac{S^2}{n}} \leq d.$$
	 Solving this inequality for n yields
	 \begin{equation*}
	 n = \frac{(\frac{zs}{d})^2}{1 + (\frac{zs}{d})^2/N}= \frac{n_0}{1 + \frac{n_0}{N}}
	 \end{equation*}
	 where $n_0 = ( \frac{zs}{d} ) ^2$ and is the sample size calculation for an infinite population, $z$ is the critical calue from a $N(0,1)$ distribution.
\vfill

Rounding up the value of $n$ yields the desired sample size.
\newpage

If the population size $N$ is very large, then 1/N $\approx 0$. In this case, $n \approx n_0$. This is the typical formula for infinite sample settings.
\vfill
There remains one major problem. 
\vfill
\vfill
\vfill

##### Sample size for $t$

Goal: Estimate the SRS size required so the probability that the difference between the estimator $\hat{t} = N \bar{y}$ and the population total $t$ does not exceed a maximum allowable difference $d$ is at most $\alpha$.
\vfill

Mathematically, find $n$ such that $Pr(|\hat{t} - t| > d) < \alpha$ for a specified maximum allowable difference $d$.
\vfill

Assuming $N \bar{y}$ is approximatly normally distributed, this is equivalent to finding $n$ so that the margin of error is $z_{\alpha/2} \sqrt{N(N-n) \frac{S^2}{n}} \leq d$ Solving this inequality for $n$ yields
	\begin{equation*}
	n = \frac{n_0N^2}{1+Nn_0} = \frac{1}{\frac{1}{N^2n_0}+\frac{1}{N}}.
	\end{equation*}
\vfill
\newpage

##### Sample size for $p$
Goal: Estimate the SRS size required so the probability that the difference between the sample proportion $\hat{p}$ and the population proportion $p$ does not exceed a maximum allowable difference $d$ is at most $\alpha.$
\vfill

Mathematically, find $n$ such that $Pr(|\hat{p}-p| > d) \leq \alpha$ for a specified maximum allowable difference d.
\vfill

Assuming $\hat{p}$ is approximately normally distributed, this is equivalent to finding n so that the margin of error $z_{\alpha/2} \sqrt{\left(\frac{N-n}{N-1}\right) \frac{p(1-p)}{n}} \leq d$.
\vfill

Solving this inequality for n yields
	$$ n \approx \frac{1}{\frac{1}{n_0} +\frac{1}{N}}$$
	where $n_0 = \frac{z_{\alpha/2}^2 p(1-p)}{d^2}$
\vfill

Unfortunately, the sample size formulas assume you know the population proportion $p$, the quantity you are trying to estimate. 

\vfill

Note that the standard deviation of $\hat{p}$ is largest when $p=1/2$. Thus it is conservative to use $p=1/2$ if there is no prior reasonable estimate.
\vfill

#### Sample Size Calculations in R
```{r}
#### Sample Size Calculations for Proportions
d <- .1 # maximum allowable difference
alpha <- .05 # 95% confidence interval
N <- 500 # population size
p <- .5 # use p= 1/2 for conservative estimate
n.0 <- (qnorm(1-alpha/2)^2 * p *(1-p)) / d^2
n = round(1 / (1/n.0 + 1/N))
```

For an example where the population size is `r N` and the goal is to estimate the proportion with a maximum allowable difference of `r d`, the required sample size is `r n`. 
